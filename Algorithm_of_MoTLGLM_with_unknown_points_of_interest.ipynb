{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824481ba",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28404bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import linalg as la\n",
    "from torch import digamma, log, sqrt, exp\n",
    "from torch.special import gammaln, ndtr\n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "import math\n",
    "import numpy\n",
    "from numpy import linalg as LA\n",
    "import pandas\n",
    "from copy import deepcopy\n",
    "from scipy import special\n",
    "from tqdm import tqdm\n",
    "from scipy.stats import qmc\n",
    "from scipy.special import gammaincinv\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import root_mean_squared_error, mean_squared_error, mean_poisson_deviance\n",
    "\n",
    "import import_ipynb\n",
    "import data_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36bc18e-252f-41c6-8a76-f6148fecf55a",
   "metadata": {},
   "source": [
    "#### 2. Poisson regression with n_points > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bc715-d6f4-4a92-9e9f-3dea537d9a08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Minimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6362fe-90b8-4bd7-81d5-3a2c54a4ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_reg_gn(initial_parameters, X, y, n_points, learning_rate, Lambda, max_iters, weights, print_flag):\n",
    "    # data shape\n",
    "    n_samples, n_features = X.shape\n",
    "    # get initial_parameters\n",
    "    C, Epsilon, K_p, TAU_p, K, TAU, W, RHO, MU, SIGMA, w_ext, rho_ext, mu_ext, sigma_ext = initial_parameters\n",
    "\n",
    "    optimizer = torch.optim.NAdam([{'params': [C, W, MU, w_ext, mu_ext]},\n",
    "                                   {'params': [Epsilon, RHO, SIGMA, rho_ext, sigma_ext], 'lr': 0.5 * learning_rate},\n",
    "                                   {'params': [K, TAU], 'lr': 20 * learning_rate}\n",
    "                                  ], lr=learning_rate)\n",
    "\n",
    "    terminated_conv, old_risk_bound = 0, numpy.finfo(numpy.float64).max\n",
    "    norm_X_square = la.norm(X, dim=1)**2\n",
    "    norm_X_square[norm_X_square > 10] = 10\n",
    "    \n",
    "    mask = y!=0\n",
    "    y_reshape_mask = (y.reshape(n_samples, 1))[mask]\n",
    "    Max_, max_ = 1000 * torch.ones(n_points), torch.ones(n_points)\n",
    "\n",
    "    dim, n_row = 1, 60\n",
    "    sampler = qmc.Halton(dim, scramble=True, seed=0)\n",
    "    x_halton = sampler.random(n_row)\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # compute Mat_Omega\n",
    "        noncentral_parameter = (la.norm((C.reshape(n_points, 1, n_features) - X), axis=2) / Epsilon.reshape(n_points,1))**2\n",
    "        inv_deno = sqrt(4.5 * n_features + (4.5 * noncentral_parameter**2 / (n_features + 2 * noncentral_parameter)))\n",
    "\n",
    "        # Mat_Loss_in, Loss_ext\n",
    "        Mat_Loss_in = torch.zeros(n_samples, n_points)\n",
    "        Mat_Loss_in[mask] = y_reshape_mask * (log(y_reshape_mask /  exp( torch.matmul(X[mask], W.T) + MU )) - 1)\n",
    "        Mat_Loss_in = (Mat_Loss_in + exp( torch.matmul(X, W.T) + MU + 0.5 * (norm_X_square.reshape(n_samples, 1) * RHO**2 + SIGMA**2) )).T\n",
    "\n",
    "        Loss_ext = torch.zeros(n_samples)\n",
    "        Loss_ext[mask] = y[mask] * (log(y[mask] /  exp( torch.matmul(X[mask], w_ext) + mu_ext )) - 1)\n",
    "        Loss_ext += exp( torch.matmul(X, w_ext) + mu_ext + 0.5 * (norm_X_square * rho_ext**2 + sigma_ext**2) )\n",
    "        \n",
    "        # compute Mat_Omega, Mat_Omega_log\n",
    "        BETA = gammaincinv(K, x_halton) / TAU\n",
    "        Mat_Omega = ndtr( (((n_features + noncentral_parameter)**(-1/3) * inv_deno) * torch.ones(n_row, 1, 1)) * ((BETA / Epsilon).reshape(n_row, n_points, 1))**(2/3) - ((inv_deno - inv_deno**(-1)) * torch.ones(n_row, 1, 1)) )\n",
    "        Mat_Omega, Mat_Omega_log = torch.mean(Mat_Omega, axis=0), torch.mean((log(BETA.reshape(n_row, n_points, 1)) * Mat_Omega), axis=0)\n",
    "        \n",
    "        # compute KL divergence\n",
    "        kl = - n_features * math.log(torch.prod(RHO) * rho_ext * torch.prod(Epsilon)) - math.log(torch.prod(SIGMA) * sigma_ext) + 0.5 * (n_features * (torch.sum(RHO**2) + rho_ext**2 + torch.sum(Epsilon**2)) + (torch.sum(SIGMA**2) + sigma_ext**2) - ((2*n_features + 1) * n_points + (n_features + 1)))\n",
    "        kl = kl + 0.5 * ( (la.norm(W, axis=1)**2).sum() + (MU**2).sum() + la.norm(w_ext)**2 + mu_ext**2 + (la.norm(C, axis=1)**2).sum() )\n",
    "        kl = kl + ( (K - K_p) * digamma(K) + gammaln(K_p) - gammaln(K) + K_p * log(TAU / TAU_p) + K * ((TAU_p - TAU) / TAU) ).sum()\n",
    "\n",
    "        # compute risk_bound\n",
    "        loss = torch.sum(Mat_Omega * Mat_Loss_in, axis=0) + torch.prod(1 - Mat_Omega, axis=0) * Loss_ext\n",
    "        risk_bound = torch.mean(2 * loss * weights) + (1 / Lambda) * kl\n",
    "    \n",
    "        # compute gradient over all parameters but K\n",
    "        risk_bound.backward()\n",
    "        \n",
    "        # compute gradient over K parameter\n",
    "        Mat_Omega_complement_modified = torch.where((1 - Mat_Omega) == 0, 1e-4, (1 - Mat_Omega))\n",
    "        Mat_Omega_prod = torch.prod((torch.zeros(n_points, 1, n_samples) + Mat_Omega_complement_modified), axis=1) / Mat_Omega_complement_modified\n",
    "        grad_K = ((log(TAU) - digamma(K)).reshape(n_points,1) * Mat_Omega + Mat_Omega_log) * (Mat_Loss_in - Mat_Omega_prod * (torch.ones(n_points,1) * Loss_ext))\n",
    "        K.grad = torch.mean(2 * grad_K * (torch.ones(n_points,1) * weights.reshape(n_samples)), axis=1) + (1 / Lambda) * ((K - K_p) * torch.polygamma(1, K) + ((TAU_p - TAU) / TAU))\n",
    "\n",
    "        optimizer.step()                        # update gradient\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            K[:] = torch.clamp(K, min=K_p, max=Max_)                                  # contraint on K_p < K < Max_\n",
    "            TAU[:] = torch.clamp(TAU, min=TAU_p, max=Max_)                            # contraint on TAU_p < TAU < Max_\n",
    "\n",
    "            Epsilon[:] = torch.clamp(torch.abs(Epsilon), max=max_)                    # contraint on RHO_0 > 0\n",
    "            RHO[:] = torch.clamp(torch.abs(RHO), max=max_)                            # contraint on RHO>0\n",
    "            SIGMA[:] = torch.clamp(torch.abs(SIGMA), max=max_)                        # contraint on SIGMA>0\n",
    "            rho_ext[:] = torch.clamp(torch.abs(rho_ext), max=torch.tensor([1.0]))     # contraint on rho_ext>0\n",
    "            sigma_ext[:] = torch.clamp(torch.abs(sigma_ext), max=torch.tensor([1.0])) # contraint on sigma_ext>0\n",
    "\n",
    "            # gradients and risk bound stop criterion\n",
    "            if (i+1)%40 == 0 :\n",
    "                if ((risk_bound / old_risk_bound) >= 0.95) or ((K / TAU**2).sum() > n_points) :\n",
    "                    terminated_conv = 1\n",
    "                    break;\n",
    "                else:\n",
    "                    old_risk_bound = risk_bound\n",
    "                    \n",
    "    if print_flag:\n",
    "        if terminated_conv != 0:\n",
    "            print(f'Converged in {i+1} iteration...')\n",
    "        else:\n",
    "            print(f'Terminated in {i+1} iteration... because maximum number of iterations has been exceeded') \n",
    "    return C.detach().numpy(), Epsilon.detach().numpy(), K.detach().numpy(), TAU.detach().numpy(), W.detach().numpy(), RHO.detach().numpy(), MU.detach().numpy(), SIGMA.detach().numpy(), w_ext.detach().numpy(), rho_ext.detach().numpy()[0], mu_ext.detach().numpy()[0], sigma_ext.detach().numpy()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff9b29d-4503-4151-93a1-82177c6422b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Definition of mixture models class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7a3793-f833-43dd-9e6a-8b4e2bf55250",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pac_bayes_reg:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, C_ini=None, Epsilon_ini=None, K_ini=None, TAU_ini=None, K_p=None, TAU_p=None, max_iters=1000, weights=None, print_flag=False):\n",
    "        self.lr = learning_rate                                    # the step size at each iteration\n",
    "        self.Lambda = lambda_reg                                   # the regularization rate (lambda_reg should be higher than 1)\n",
    "        self.C_ini, self.Epsilon_ini = C_ini, Epsilon_ini          # initial mean vector C of the points of interest and its standard error Epsilon_ini\n",
    "        self.K_ini, self.TAU_ini = K_ini, TAU_ini                  # initial values of the shape and rate of locality parameter beta to find (beta = k/tau)\n",
    "        self.K_p, self.TAU_p = K_p, TAU_p                          # prior value of the shape and rate (gamma distribution parameter)\n",
    "        self.max_iters = max_iters                                 # maximum number of iterations before stopping independently of any early stopping\n",
    "        self.weights_vet = weights                                 # to take into account the skewed distribution of the classes (if necessary)\n",
    "        self.print_flag = print_flag                               # flag to print some infos\n",
    "        self.C, self.Epsilon = None, None                          # point of interest and it's standard deviation\n",
    "        self.W, self.RHO, self.MU, self.SIGMA = None, None, None, None                 # model parameters to be find in n localities\n",
    "        self.w_ext, self.rho_ext, self.mu_ext, self.sigma_ext = None, None, None, None # model external parameters to be find out of n localities\n",
    "        self.K, self.TAU, self.BETA = None, None, None                                 # the locality parameters beta to find beta = (k, tau)\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # data shape and number of vicinity points\n",
    "        n_samples, n_features = X.shape\n",
    "        n_points = self.K_p.size\n",
    "\n",
    "        # compute weight\n",
    "        self.weights_vet = numpy.ones(n_samples) if self.weights_vet is None else self.weights_vet\n",
    "\n",
    "        # initialize parameters\n",
    "        if self.C_ini is None:\n",
    "            self.C_ini, self.Epsilon_ini = torch.randn(n_points, n_features), torch.rand(n_points)\n",
    "            self.K_ini, self.TAU_ini = 50 * torch.rand(n_points), 50 * torch.rand(n_points)\n",
    "            while( (self.K_ini < torch.from_numpy(self.K_p)).sum()!= 0 or (self.TAU_ini < torch.from_numpy(self.TAU_p)).sum()!= 0 ):\n",
    "                self.K_ini, self.TAU_ini = 50 * torch.rand(n_points), 50 * torch.rand(n_points)\n",
    "\n",
    "        self.C, self.Epsilon = deepcopy(self.C_ini), deepcopy(self.Epsilon_ini)\n",
    "        self.K, self.TAU = deepcopy(self.K_ini), deepcopy(self.TAU_ini)\n",
    "        \n",
    "        self.W, self.RHO = torch.zeros(n_points, n_features), torch.ones(n_points)\n",
    "        self.MU, self.SIGMA = torch.zeros(n_points), torch.ones(n_points)\n",
    "        self.w_ext, self.rho_ext = torch.zeros(n_features), torch.tensor([1.0])\n",
    "        self.mu_ext, self.sigma_ext = torch.tensor([0.0]), torch.tensor([1.0])\n",
    "\n",
    "        # concatenate (initialize parameters)\n",
    "        starting_point = (self.C.requires_grad_(), self.Epsilon.requires_grad_(), torch.from_numpy(self.K_p), torch.from_numpy(self.TAU_p), self.K, self.TAU.requires_grad_(), self.W.requires_grad_(), self.RHO.requires_grad_(), self.MU.requires_grad_(), self.SIGMA.requires_grad_(), self.w_ext.requires_grad_(), self.rho_ext.requires_grad_(), self.mu_ext.requires_grad_(), self.sigma_ext.requires_grad_())\n",
    "        # convert data to tensor\n",
    "        X_, y_, self.weights_vet = torch.from_numpy(X), torch.from_numpy(y), torch.from_numpy(self.weights_vet)\n",
    "        # minimized params\n",
    "        self.C, self.Epsilon, self.K, self.TAU, self.W, self.RHO, self.MU, self.SIGMA, self.w_ext, self.rho_ext, self.mu_ext, self.sigma_ext = minimize_reg_gn(starting_point, X_, y_, n_points, self.lr, self.Lambda, self.max_iters, self.weights_vet, self.print_flag)\n",
    "        self.BETA = numpy.round(self.K / self.TAU, 6)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        n_points = self.C.shape[0]\n",
    "        Mat_dists = LA.norm((self.C.reshape(n_points, 1, self.C.shape[1]) - X), axis=2)\n",
    "\n",
    "        W_params = numpy.concatenate((self.W, self.w_ext.reshape(1, self.w_ext.size)), axis=0)\n",
    "        MU_params = numpy.concatenate((self.MU, numpy.array([self.mu_ext])), axis=0)\n",
    "        approx = (numpy.matmul(X, W_params.T) + MU_params).T\n",
    "        \n",
    "        # get idx_W for each samples\n",
    "        idx_W = numpy.argmin((Mat_dists + numpy.where(Mat_dists <= self.BETA.reshape(n_points,1), 0, 1)), axis=0)\n",
    "        number_of_overlap_region = numpy.sum(numpy.where(Mat_dists <= self.BETA.reshape(n_points,1), 1, 0), axis=0)\n",
    "        idx_W[numpy.where(number_of_overlap_region == 0)[0]] = n_points\n",
    "\n",
    "        mask = numpy.nonzero( (numpy.ones((n_points+1,1)) * idx_W) == (numpy.arange(n_points+1)).reshape(n_points+1,1) )\n",
    "        mask = (mask[0][numpy.argsort(mask[1])], numpy.sort(mask[1]))\n",
    "        return numpy.exp(approx[mask])\n",
    "    \n",
    "    def mse_score(self, X, y):\n",
    "        return numpy.round(mean_squared_error(y_true=y, y_pred=self.predict(X)), 6)\n",
    "\n",
    "    def mpd_score(self, X, y):\n",
    "        return numpy.round(mean_poisson_deviance(y_true=y, y_pred=self.predict(X)), 6)\n",
    "\n",
    "    def risk_bound(self, X, y):\n",
    "        # get data shape\n",
    "        n_samples, n_features = X.shape\n",
    "        n_points = self.K.size\n",
    "        \n",
    "        dim, n_row = 1, 60\n",
    "        sampler = qmc.Halton(dim, scramble=True, seed=0)\n",
    "        x_halton = sampler.random(n_row)\n",
    "\n",
    "        # compute Mat_Omega\n",
    "        noncentral_parameter = (LA.norm((self.C.reshape(n_points, 1, n_features) - X), axis=2) / self.Epsilon.reshape(n_points,1))**2\n",
    "        inv_deno = numpy.sqrt(4.5 * n_features + (4.5 * noncentral_parameter**2 / (n_features + 2 * noncentral_parameter)))\n",
    "        \n",
    "        BETA = gammaincinv(self.K, x_halton) / self.TAU\n",
    "        Mat_Omega = special.ndtr( (((n_features + noncentral_parameter)**(-1/3) * inv_deno) * numpy.ones((n_row, 1, 1))) * ((BETA / self.Epsilon).reshape((n_row, n_points, 1)))**(2/3) - ((inv_deno - inv_deno**(-1)) * numpy.ones((n_row, 1, 1))) )\n",
    "        Mat_Omega = numpy.mean(Mat_Omega, axis=0)\n",
    "        \n",
    "        # Mat_Loss_in and Loss_ext\n",
    "        norm_X_square = LA.norm(X, axis=1)**2\n",
    "        norm_X_square[norm_X_square > 10] = 10\n",
    "        mask = y!=0\n",
    "        \n",
    "        y_reshape = y.reshape(n_samples, 1)\n",
    "        Mat_Loss_in = numpy.zeros((n_samples, n_points))\n",
    "        Mat_Loss_in[mask] = y_reshape[mask] * (numpy.log(y_reshape[mask] /  numpy.exp( numpy.dot(X[mask], self.W.T) + self.MU )) - 1)\n",
    "        Mat_Loss_in = (Mat_Loss_in + numpy.exp( numpy.dot(X, self.W.T) + self.MU + 0.5 * (norm_X_square.reshape(n_samples, 1) * self.RHO**2 + self.SIGMA**2) )).T\n",
    "\n",
    "        Loss_ext = numpy.zeros(n_samples)\n",
    "        Loss_ext[mask] = y[mask] * (numpy.log(y[mask] /  numpy.exp( numpy.dot(X[mask], self.w_ext) + self.mu_ext )) - 1)\n",
    "        Loss_ext += numpy.exp( numpy.dot(X, self.w_ext) + self.mu_ext + 0.5 * (norm_X_square * self.rho_ext**2 + self.sigma_ext**2) )\n",
    "\n",
    "        # compute empirical risk\n",
    "        ER = numpy.mean(2 * (numpy.sum(Mat_Omega * Mat_Loss_in, axis=0) + numpy.prod(1 - Mat_Omega, axis=0) * Loss_ext))\n",
    "        \n",
    "        # compute KL divergence\n",
    "        gaussian_kl = - n_features * math.log(numpy.prod(self.RHO) * self.rho_ext * numpy.prod(self.Epsilon)) - math.log(numpy.prod(self.SIGMA) * self.sigma_ext) + 0.5 * (n_features * (numpy.sum(self.RHO**2) + self.rho_ext**2 + numpy.sum(self.Epsilon**2)) + (numpy.sum(self.SIGMA**2) + self.sigma_ext**2) - ((2*n_features + 1) * n_points + (n_features + 1)))\n",
    "        gaussian_kl += 0.5 * (numpy.sum(LA.norm(self.W, axis=1)**2, axis=0) + numpy.sum(self.MU**2, axis=0) + LA.norm(self.w_ext)**2 + self.mu_ext**2 + numpy.sum(LA.norm(self.C, axis=1)**2, axis=0))\n",
    "        gamma_kl = numpy.sum( (self.K - self.K_p) * special.psi(self.K) + special.gammaln(self.K_p) - special.gammaln(self.K) + self.K_p * numpy.log(self.TAU / self.TAU_p) + self.K * ((self.TAU_p - self.TAU) / self.TAU) )\n",
    "        return ER, ER + (1 / self.Lambda) * (gaussian_kl + gamma_kl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72ebb96-ea8f-466d-b600-a25eb4fcaa5f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Choice of Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0b20dd-2af6-4aef-8e01-ce5a6ffd07c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_mixture_reg(X_train, X_val, y_train, y_val, T=10, lr=0.01, lambda_param=1000, K_p=None, TAU_p=None, max_iters=1000, weights=None, print_flag=False):\n",
    "    best_exist = False\n",
    "    old_train_score = numpy.finfo(numpy.float64).max\n",
    "    for i in range(T):\n",
    "        mixture_reg = Pac_bayes_reg(learning_rate=lr, lambda_reg=lambda_param, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights, print_flag=print_flag)\n",
    "        mixture_reg.fit(X_train, y_train)\n",
    "        new_train_score = mixture_reg.mpd_score(X_train, y_train)\n",
    "\n",
    "        if (new_train_score < old_train_score) and ((mixture_reg.K / mixture_reg.TAU**2).sum() < len(K_p)):\n",
    "            Initial_parameters = (mixture_reg.C_ini, mixture_reg.Epsilon_ini, mixture_reg.K_ini, mixture_reg.TAU_ini)\n",
    "            val_score, old_train_score = mixture_reg.mpd_score(X_val, y_val), new_train_score\n",
    "            best_exist = True\n",
    "            \n",
    "    if best_exist == False:\n",
    "        Initial_parameters = None\n",
    "        val_score = numpy.finfo(numpy.float64).max\n",
    "    return val_score, Initial_parameters\n",
    "\n",
    "def lambda_validation_reg(X_train, X_val, y_train, y_val, lr=0.01, K_p=None, TAU_p=None, max_iters=1000, weights=None): \n",
    "    T, old_val_score = 10 * len(K_p), numpy.finfo(numpy.float64).max\n",
    "    lambda_params = numpy.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]) * X_train.shape[0]\n",
    "    \n",
    "    i = 0\n",
    "    pbar = tqdm(desc=\"Tuning Lambda (\"+str(T)+\" random restarts for each lambda) : \", total=len(lambda_params), position=0)\n",
    "        \n",
    "    while(i < len(lambda_params)) :\n",
    "        lambda_param = lambda_params[i]\n",
    "        torch.manual_seed(lambda_param)\n",
    "        new_val_score, new_Initial_parameters = build_mixture_reg(X_train, X_val, y_train, y_val, T=T, lr=lr, lambda_param=lambda_param, K_p=K_p, TAU_p=TAU_p, \n",
    "                                                                  max_iters=max_iters, weights=weights, print_flag=False)\n",
    "        if (new_val_score <= old_val_score):\n",
    "            lambda_param_, Initial_parameters, old_val_score = lambda_param, new_Initial_parameters, new_val_score\n",
    "        \n",
    "        pbar.update(1)\n",
    "        i = i + 1\n",
    "\n",
    "        elapsed = pbar.format_dict[\"elapsed\"]\n",
    "        if elapsed > 1800:\n",
    "            break;\n",
    "            \n",
    "    pbar.close()\n",
    "    return lambda_param_, Initial_parameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d24b9f-0568-4f79-ac29-07d5db9365be",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Mixture Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9d928e-e95c-43a6-91b7-face77810fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixture_reg(data, target_name, n_points=2, train_size=0.75, lr=0.01, lambda_param=1e-3, max_iters=1000, lambda_validation=False, times=1, check_multicollinearity=True, return_flag='simple'):\n",
    "    # uncouping X and y reg\n",
    "    X, y = data_analysis.uncouping_x_y_reg(data, target_name)\n",
    "\n",
    "    # get weights\n",
    "    weights = None\n",
    "    print('***************** Mixtures of transparent local models without given points of interest *****************')\n",
    "    print(f'Training_set = {round((train_size * 100))}%, Validation_set = {round(((1 - train_size)/2) * 100)}%, Test_set = {round(((1 - train_size)/2) * 100)}%, n_points = {n_points}, weights = {weights}, lambda_validation = {lambda_validation}, times = {times}, check_multicollinearity = {check_multicollinearity}')\n",
    "\n",
    "    for i in tqdm(numpy.arange(times), desc=\"For Random Data Split = \"+str(times)+\" …\", total=times, position=0):\n",
    "        # split the dataset X into the training set X_train and temporary set X_temp\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size = train_size, random_state=i)\n",
    "        # split the dataset X_temp into the validation set X_val and testing set X_test\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size = 0.5, random_state=0)\n",
    "        X_train, X_val, X_test = data_analysis.reset_index_data(data_1=X_train, data_2=X_val, data_3=X_test, data_4=None)    \n",
    "\n",
    "        # data encoding (OneHotEncoder encoding for category variables) and Standardscaler scaling\n",
    "        X_train_enc, X_val_enc, X_test_enc = data_analysis.data_processing(xtrain=X_train.copy(), ytrain=y_train.copy(), xtest_1=X_val.copy(), xtest_2=X_test.copy(), xtest_3=None, check_multicollinearity=check_multicollinearity)\n",
    "\n",
    "        # define gamma_priors\n",
    "        K_p, TAU_p = 2.0 * numpy.ones(n_points), (1 / 10) * numpy.ones(n_points)\n",
    "        \n",
    "        # finding the best lambda by cross validation on data\n",
    "        if lambda_validation == True:\n",
    "            lambda_param, Initial_parameters = lambda_validation_reg(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), lr=lr, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights)\n",
    "        else :\n",
    "            torch.manual_seed(lambda_param)\n",
    "            MSE_score, Initial_parameters = build_mixture_reg(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), T=10 * n_points, lr=lr, lambda_param=lambda_param, K_p=K_p, TAU_p=TAU_p, max_iters=max_iters, weights=weights, print_flag=False)\n",
    "            \n",
    "        # fitting model\n",
    "        mixture_reg = Pac_bayes_reg(learning_rate=lr, lambda_reg=lambda_param, C_ini=Initial_parameters[0], Epsilon_ini=Initial_parameters[1], K_ini=Initial_parameters[2], TAU_ini=Initial_parameters[3], K_p=K_p.copy(), TAU_p=TAU_p.copy(), max_iters=max_iters, weights=weights, print_flag=False)\n",
    "        mixture_reg.fit(X_train_enc, y_train)\n",
    "        \n",
    "        # prediction\n",
    "        y_c_preds = mixture_reg.predict(mixture_reg.C)\n",
    "        y_train_preds = mixture_reg.predict(X_train_enc)\n",
    "        y_val_preds = mixture_reg.predict(X_val_enc)\n",
    "        y_test_preds = mixture_reg.predict(X_test_enc)\n",
    "        \n",
    "        # compute risk for each dataset\n",
    "        risk_bound_set = []\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_train_enc, y_train))\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_val_enc, y_val))\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_test_enc, y_test))\n",
    "        risk_bound_set = numpy.round(numpy.array(risk_bound_set), 4)\n",
    "\n",
    "        # get summary\n",
    "        summary_random = results_summary_reg(y_train, y_val, y_test, y_train_preds, y_val_preds, y_test_preds, risk_bound_set[:,0], risk_bound_set[:,1])\n",
    "        print(f'summary_random={summary_random}')\n",
    "        if (i == 0):\n",
    "            summary_random_state = summary_random\n",
    "            C_argsort = mixture_reg.C[:, 0].argsort()\n",
    "            \n",
    "            if return_flag != 'simple':\n",
    "                W_random_state, MU_random_state, w_ext_random_state, mu_ext_random_state = mixture_reg.W[C_argsort], mixture_reg.MU[C_argsort], mixture_reg.w_ext, mixture_reg.mu_ext\n",
    "                C_random_state, BETA_random_state = mixture_reg.C[C_argsort], mixture_reg.BETA[C_argsort]\n",
    "            \n",
    "        else:\n",
    "            summary_random_state += summary_random\n",
    "            C_argsort = mixture_reg.C[:, 0].argsort()\n",
    "            \n",
    "            if return_flag != 'simple':\n",
    "                W_random_state += mixture_reg.W[C_argsort]\n",
    "                MU_random_state += mixture_reg.MU[C_argsort]\n",
    "                w_ext_random_state += mixture_reg.w_ext\n",
    "                mu_ext_random_state += mixture_reg.mu_ext\n",
    "                C_random_state += mixture_reg.C[C_argsort]\n",
    "                BETA_random_state += mixture_reg.BETA[C_argsort]\n",
    "                \n",
    "        print(f'C = {mixture_reg.C[C_argsort]}, Epsilon = {numpy.round(mixture_reg.Epsilon[C_argsort], 6)}, lambda_param = {lambda_param}')\n",
    "        print(f'K = {mixture_reg.K[C_argsort]}, TAU = {numpy.round(mixture_reg.TAU[C_argsort], 6)}')\n",
    "        print(f'W = {mixture_reg.W[C_argsort]}, MU = {numpy.round(mixture_reg.MU[C_argsort], 6)}')\n",
    "        print(f'w_ext = {mixture_reg.w_ext}, mu_ext = {round(mixture_reg.mu_ext, 6)}')\n",
    "        print(f'RHO = {mixture_reg.RHO[C_argsort]}, SIGMA = {numpy.round(mixture_reg.SIGMA[C_argsort], 6)}')\n",
    "        print(f'rho_ext = {mixture_reg.rho_ext}, sigma_ext = {round(mixture_reg.sigma_ext, 6)}')\n",
    "\n",
    "    summary = (summary_random_state / times).astype('float64')\n",
    "    summary = summary.round(4)\n",
    "    print(f'*********** END ***********')\n",
    "    \n",
    "    if return_flag=='simple':\n",
    "        return lambda_param, summary\n",
    "    else :\n",
    "        W, MU, w_ext, mu_ext = (W_random_state / times), (MU_random_state / times), (w_ext_random_state / times), (mu_ext_random_state / times)\n",
    "        C, BETA = (C_random_state / times), (BETA_random_state / times)\n",
    "\n",
    "        print(f'C = {C}, BETA = {BETA}')\n",
    "        print(f'W = {W}, MU = {numpy.round(MU, 6)}')\n",
    "        print(f'w_ext = {w_ext}, mu_ext = {round(mu_ext, 6)}')\n",
    "\n",
    "        return lambda_param, C, W, w_ext, MU, mu_ext, BETA, summary, X_train_enc, X_test_enc, y_train.copy(), y_test.copy(), y_train_preds.copy(), y_test_preds.copy(), y_c_preds\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae4c0b-1674-4e82-8775-72377134e625",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Performance measurement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9c6ec5-950c-4cd8-9bf6-112fa2b666c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_summary_reg(ytrain_true, yval_true, ytest_true, ytrain_pred, yval_pred, ytest_pred, Gibbs_risk_set, risk_bound_set):\n",
    "    # for global model\n",
    "    Summary_index = ['Training set', 'Validation set', 'Testing set']\n",
    "    Summary_columns = ['RMSE', 'MSE', 'MPD', 'Gibbs_risk', 'Risk_bound']\n",
    "    Summary_results = pandas.DataFrame(index=Summary_index, columns=Summary_columns)\n",
    "\n",
    "    # performance of the global model\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[0]] = round(root_mean_squared_error(ytrain_true, ytrain_pred), 4)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[1]] = round(mean_squared_error(ytrain_true, ytrain_pred), 4)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[2]] = round(mean_poisson_deviance(ytrain_true, ytrain_pred), 4)\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[3]] = Gibbs_risk_set[0]\n",
    "    Summary_results.loc[Summary_index[0], Summary_columns[4]] = risk_bound_set[0]\n",
    "    \n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[0]] = round(root_mean_squared_error(yval_true, yval_pred), 4)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[1]] = round(mean_squared_error(yval_true, yval_pred), 4)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[2]] = round(mean_poisson_deviance(yval_true, yval_pred), 4)\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[3]] = Gibbs_risk_set[1]\n",
    "    Summary_results.loc[Summary_index[1], Summary_columns[4]] = risk_bound_set[1]\n",
    "    \n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[0]] = round(root_mean_squared_error(ytest_true, ytest_pred), 4)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[1]] = round(mean_squared_error(ytest_true, ytest_pred), 4)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[2]] = round(mean_poisson_deviance(ytest_true, ytest_pred), 4)\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[3]] = Gibbs_risk_set[2]\n",
    "    Summary_results.loc[Summary_index[2], Summary_columns[4]] = risk_bound_set[2]\n",
    "    \n",
    "    return Summary_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6ddad7-fcc0-4402-93c4-26c800150035",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### 3 Poisson regression with n_points=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d3f0c3-57d8-409c-ba7e-34723a9ccb61",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Minimization objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d709c13-3f9b-4a7b-955c-b9b7d08c4e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_objective_n_0(initial_parameters, X, y, learning_rate, Lambda, max_iters, weights, print_flag):\n",
    "    # data shape\n",
    "    n_samples, n_features = X.shape\n",
    "    # get initial_parameters\n",
    "    w, rho, mu, sigma = initial_parameters\n",
    "\n",
    "    optimizer = torch.optim.NAdam([{'params': [w, mu]}, {'params': [rho, sigma], 'lr': 0.5 * learning_rate}], lr=learning_rate)\n",
    "\n",
    "    terminated_conv, old_risk_bound = 0, numpy.finfo(numpy.float64).max\n",
    "    norm_X_square = la.norm(X, dim=1)**2\n",
    "    norm_X_square[norm_X_square > 10] = 10\n",
    "    mask = y!=0\n",
    "    \n",
    "    for i in range(max_iters):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Loss\n",
    "        Loss = torch.zeros(n_samples)\n",
    "        Loss[mask] = y[mask] * (log(y[mask] /  exp( torch.matmul(X[mask], w) + mu )) - 1)\n",
    "        Loss += exp( torch.matmul(X, w) + mu + 0.5 * (norm_X_square * rho**2 + sigma**2) )\n",
    "        \n",
    "        # compute KL divergence\n",
    "        kl = - n_features * math.log(rho) - math.log(sigma) + 0.5 * ( n_features * rho**2 + sigma**2 - (n_features + 1) ) + 0.5 * ( la.norm(w)**2 + mu**2 )\n",
    "\n",
    "        # compute risk_bound\n",
    "        risk_bound = torch.mean(2 * Loss * weights) + (1 / Lambda) * kl\n",
    "        \n",
    "        # compute gradient over all parameters\n",
    "        risk_bound.backward()\n",
    "        \n",
    "        # update gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rho[:] = torch.clamp(torch.abs(rho), max=torch.tensor([1.0]))     # contraint on rho>0\n",
    "            sigma[:] = torch.clamp(torch.abs(sigma), max=torch.tensor([1.0])) # contraint on sigma>0\n",
    "\n",
    "            # stop criterion\n",
    "            if (i+1)%40 == 0 :\n",
    "                if (risk_bound / old_risk_bound) >= 0.95 :\n",
    "                    terminated_conv = 1\n",
    "                    break;\n",
    "                else:\n",
    "                    old_risk_bound = risk_bound\n",
    "                    \n",
    "    if print_flag:\n",
    "        if terminated_conv != 0:\n",
    "            print(f'Converged in {i+1} iteration...')\n",
    "        else:\n",
    "            print(f'Terminated in {i+1} iteration... because maximum number of iterations has been exceeded') \n",
    "    return w.detach().numpy(), rho.detach().numpy()[0], mu.detach().numpy()[0], sigma.detach().numpy()[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b82b7b-aa23-47a3-8da7-5d42441d77c4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Definition of mixture models class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e5582-24c1-4c61-959a-7a25729a9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pac_bayes_reg_n_0:\n",
    "    def __init__(self, learning_rate=0.01, lambda_reg=0.1, max_iters=1000, weights=None, print_flag=False):\n",
    "        self.lr = learning_rate                                    # the step size at each iteration\n",
    "        self.Lambda = lambda_reg                                   # the regularization rate (lambda_reg should be higher than 1)\n",
    "        self.max_iters = max_iters                                 # maximum number of iterations before stopping independently of any early stopping\n",
    "        self.weights_vet = weights                                 # to take into account the skewed distribution of the classes (if necessary)\n",
    "        self.print_flag = print_flag                               # flag to print some infos\n",
    "        self.w, self.rho, self.mu, self.sigma = None, None, None, None # model external parameters to be find out of n localities\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        # data shape and number of vicinity points\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # compute weight\n",
    "        self.weights_vet = numpy.ones(n_samples) if self.weights_vet is None else self.weights_vet\n",
    "\n",
    "        # initialize parameters\n",
    "        self.w, self.rho = torch.zeros(n_features), torch.tensor([1.0])\n",
    "        self.mu, self.sigma = torch.tensor([0.0]), torch.tensor([1.0])\n",
    "\n",
    "        # concatenate (initialize parameters)\n",
    "        starting_point = (self.w.requires_grad_(), self.rho.requires_grad_(), self.mu.requires_grad_(), self.sigma.requires_grad_())\n",
    "        # convert data to tensor\n",
    "        X_, y_, self.weights_vet = torch.from_numpy(X), torch.from_numpy(y), torch.from_numpy(self.weights_vet)\n",
    "        # minimized params\n",
    "        self.w, self.rho, self.mu, self.sigma = minimize_objective_n_0(starting_point, X_, y_, self.lr, self.Lambda, self.max_iters, self.weights_vet, self.print_flag)\n",
    "            \n",
    "    def predict(self, X):\n",
    "        return numpy.exp(numpy.dot(X, self.w) + self.mu)\n",
    "    \n",
    "    def mse_score(self, X, y):\n",
    "        return numpy.round(mean_squared_error(y_true=y, y_pred=self.predict(X)), 6)\n",
    "    \n",
    "    def mpd_score(self, X, y):\n",
    "        return numpy.round(mean_poisson_deviance(y_true=y, y_pred=self.predict(X)), 6)\n",
    "\n",
    "    def risk_bound(self, X, y):\n",
    "        # get data shape\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Loss\n",
    "        norm_X_square = LA.norm(X, axis=1)**2\n",
    "        norm_X_square[norm_X_square > 10] = 10\n",
    "        mask = y!=0\n",
    "        \n",
    "        Loss = numpy.zeros(n_samples)\n",
    "        Loss[mask] = y[mask] * (numpy.log(y[mask] /  numpy.exp( numpy.dot(X[mask], self.w) + self.mu )) - 1)\n",
    "        Loss += numpy.exp( numpy.dot(X, self.w) + self.mu + 0.5 * (norm_X_square * self.rho**2 + self.sigma**2) )\n",
    "\n",
    "        # compute empirical risk of Gibbs\n",
    "        ER = numpy.mean(2 * Loss)\n",
    "        \n",
    "        # compute KL divergence\n",
    "        gaussian_kl = - n_features * math.log(self.rho) - math.log(self.sigma) + 0.5 * ( n_features * self.rho**2 + (self.sigma**2) - (n_features + 1) )\n",
    "        gaussian_kl += 0.5 * (LA.norm(self.w)**2 + self.mu**2)\n",
    "        return ER, ER + (1 / self.Lambda) * (gaussian_kl)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f93635-a3ed-453e-a9ef-a6ae891fd7c1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Choice of Lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179f0538-75f9-48ca-b0a1-e44e746a180f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_validation_reg_n_0(X_train, X_val, y_train, y_val, lr=0.01, max_iters=1000, weights=None): \n",
    "    old_val_score = numpy.finfo(numpy.float64).max\n",
    "    lambda_params = numpy.array([0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]) * X_train.shape[0]\n",
    "    \n",
    "    i = 0\n",
    "    pbar = tqdm(desc=\"Tuning Lambda (none random restarts for each lambda) : \", total=len(lambda_params), position=0)\n",
    "        \n",
    "    while(i < len(lambda_params)) :\n",
    "        lambda_param = lambda_params[i]\n",
    "        \n",
    "        mixture_reg = Pac_bayes_reg_n_0(learning_rate=lr, lambda_reg=lambda_param, max_iters=max_iters, weights=weights, print_flag=False)\n",
    "        mixture_reg.fit(X_train, y_train)\n",
    "        new_val_score = mixture_reg.mpd_score(X_val, y_val)\n",
    "\n",
    "        if (new_val_score <= old_val_score):\n",
    "            lambda_param_, old_val_score = lambda_param, new_val_score\n",
    "        \n",
    "        pbar.update(1)\n",
    "        i = i + 1\n",
    "\n",
    "        elapsed = pbar.format_dict[\"elapsed\"]\n",
    "        if elapsed > 1800:\n",
    "            break;\n",
    "            \n",
    "    pbar.close()\n",
    "    return lambda_param_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549e8139-4a87-432b-97f8-e8d522753ec9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "##### Mixture Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f306661a-e7cf-476a-a394-6ecc9fc9ca5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Mixture_reg_n_0(data, target_name, train_size=0.75, lr=0.01, lambda_param=1e-3, max_iters=1000, lambda_validation=False, times=1, check_multicollinearity=True, return_flag='simple'):\n",
    "    # uncouping X and y reg\n",
    "    X, y = data_analysis.uncouping_x_y_reg(data, target_name)\n",
    "    # get weights\n",
    "    weights = None\n",
    "    print('***************** Mixtures of transparent local models without given points of interest *****************')\n",
    "    print(f'Training_set = {round((train_size * 100))}%, Validation_set = {round(((1 - train_size)/2) * 100)}%, Test_set = {round(((1 - train_size)/2) * 100)}%, weights = {weights}, lambda_validation = {lambda_validation}, times = {times}, check_multicollinearity = {check_multicollinearity}')\n",
    "\n",
    "    for i in tqdm(numpy.arange(times), desc=\"For Random Data Split = \"+str(times)+\" …\", total=times, position=0):\n",
    "        # split the dataset X into the training set X_train and temporary set X_temp\n",
    "        X_train, X_temp, y_train, y_temp = train_test_split(X, y, train_size = train_size, random_state=i)\n",
    "        # split the dataset X_temp into the validation set X_val and testing set X_test\n",
    "        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, train_size = 0.5, random_state=0)\n",
    "        X_train, X_val, X_test = data_analysis.reset_index_data(data_1=X_train, data_2=X_val, data_3=X_test, data_4=None)    \n",
    "\n",
    "        # data encoding (OneHotEncoder encoding for category variables) and Standardscaler scaling\n",
    "        X_train_enc, X_val_enc, X_test_enc = data_analysis.data_processing(xtrain=X_train.copy(), ytrain=y_train.copy(), xtest_1=X_val.copy(), xtest_2=X_test.copy(), xtest_3=None, check_multicollinearity=check_multicollinearity)\n",
    "\n",
    "        # finding the best lambda by cross validation on data\n",
    "        if lambda_validation == True:\n",
    "            lambda_param = lambda_validation_reg_n_0(X_train_enc.copy(), X_val_enc.copy(), y_train.copy(), y_val.copy(), lr=lr, max_iters=max_iters, weights=weights)\n",
    "   \n",
    "        # fitting model\n",
    "        mixture_reg = Pac_bayes_reg_n_0(learning_rate=lr, lambda_reg=lambda_param, max_iters=max_iters, weights=weights, print_flag=False)\n",
    "        mixture_reg.fit(X_train_enc, y_train)\n",
    "        \n",
    "        # prediction\n",
    "        y_train_preds = mixture_reg.predict(X_train_enc)\n",
    "        y_val_preds = mixture_reg.predict(X_val_enc)\n",
    "        y_test_preds = mixture_reg.predict(X_test_enc)\n",
    "        \n",
    "        # compute risk for each dataset\n",
    "        risk_bound_set = []\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_train_enc, y_train))\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_val_enc, y_val))\n",
    "        risk_bound_set.append(mixture_reg.risk_bound(X_test_enc, y_test))\n",
    "        risk_bound_set = numpy.round(numpy.array(risk_bound_set), 4)\n",
    "\n",
    "        # get summary\n",
    "        summary_random = results_summary_reg(y_train, y_val, y_test, y_train_preds, y_val_preds, y_test_preds, risk_bound_set[:,0], risk_bound_set[:,1])\n",
    "\n",
    "        if (i == 0):\n",
    "            summary_random_state = summary_random\n",
    "        else:\n",
    "            summary_random_state += summary_random\n",
    "\n",
    "        print(f'w = {mixture_reg.w}, mu = {round(mixture_reg.mu, 6)}, lambda_param = {lambda_param}')\n",
    "        print(f'rho = {mixture_reg.rho}, sigma = {round(mixture_reg.sigma, 6)}')\n",
    "\n",
    "    summary = (summary_random_state / times).astype('float64')\n",
    "    summary = summary.round(4)\n",
    "    print(f'*********** END ***********')\n",
    "\n",
    "    \n",
    "    if return_flag=='simple':\n",
    "        return lambda_param, summary\n",
    "    else :\n",
    "        return lambda_param, summary, X_train_enc, X_test_enc, y_train.copy(), y_test.copy(), y_train_preds.copy(), y_test_preds.copy()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
